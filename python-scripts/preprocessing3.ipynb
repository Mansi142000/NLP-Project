{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.11).\n",
      "Loaded dataset from: C:\\Users\\Mansi\\.cache\\kagglehub\\datasets\\raedaddala\\top-500-600-movies-of-each-year-from-1960-to-2024\\versions\\3\\final_dataset.csv\n",
      "          id                            title  year duration MPA  rating  \\\n",
      "0  tt0073195                             Jaws  1975    2h 4m  PG     8.1   \n",
      "1  tt0073629    The Rocky Horror Picture Show  1975   1h 40m   R     7.4   \n",
      "2  tt0073486  One Flew Over the Cuckoo's Nest  1975   2h 13m   R     8.7   \n",
      "3  tt0072890                Dog Day Afternoon  1975    2h 5m   R     8.0   \n",
      "4  tt0073692                          Shampoo  1975   1h 50m   R     6.4   \n",
      "\n",
      "  votes  meta_score                                        description  \\\n",
      "0  690K        87.0  When a massive killer shark unleashes chaos on...   \n",
      "1  174K        65.0  A newly-engaged couple have a breakdown in an ...   \n",
      "2  1.1M        84.0  In the Fall of 1963, a Korean War veteran and ...   \n",
      "3  281K        86.0  Three amateur robbers plan to hold up a Brookl...   \n",
      "4   15K        65.0  On Election Day, 1968, irresponsible hairdress...   \n",
      "\n",
      "                             Movie_Link  ... opening_weekend_gross  \\\n",
      "0  https://www.imdb.com/title/tt0073195  ...            $7,061,513   \n",
      "1  https://www.imdb.com/title/tt0073629  ...                   NaN   \n",
      "2  https://www.imdb.com/title/tt0073486  ...                   NaN   \n",
      "3  https://www.imdb.com/title/tt0072890  ...                   NaN   \n",
      "4  https://www.imdb.com/title/tt0073692  ...                   NaN   \n",
      "\n",
      "  gross_worldwide gross_us_canada release_date  \\\n",
      "0    $477,916,625    $267,263,625       1975.0   \n",
      "1    $115,827,018    $112,892,319       1975.0   \n",
      "2    $109,115,366    $108,981,275       1975.0   \n",
      "3     $50,004,527     $50,000,000       1975.0   \n",
      "4     $49,407,734     $49,407,734       1975.0   \n",
      "\n",
      "                      countries_origin  \\\n",
      "0                    ['United States']   \n",
      "1  ['United Kingdom', 'United States']   \n",
      "2                    ['United States']   \n",
      "3                    ['United States']   \n",
      "4                    ['United States']   \n",
      "\n",
      "                                   filming_locations  \\\n",
      "0  [\"Water Street, Edgartown, Martha's Vineyard, ...   \n",
      "1  [\"Oakley Court, Windsor Road, Oakley Green, Wi...   \n",
      "2  ['Oregon State Mental Hospital - 2600 Center S...   \n",
      "3  ['285 Prospect Park West, Brooklyn, New York C...   \n",
      "4  [\"2270 Bowmont Drive, Beverly Hills, Californi...   \n",
      "\n",
      "                                production_companies  \\\n",
      "0  ['Zanuck/Brown Productions', 'Universal Pictur...   \n",
      "1  ['Twentieth Century Fox', 'Michael White Produ...   \n",
      "2                   ['Fantasy Films', 'N.V. Zvaluw']   \n",
      "3  ['Warner Bros.', 'Artists Entertainment Complex']   \n",
      "4  ['Persky-Bright / Vista', 'Columbia Pictures',...   \n",
      "\n",
      "                                 awards_content  \\\n",
      "0  Won 3 Oscars, 16 wins & 20 nominations total   \n",
      "1          Awards, 3 wins & 4 nominations total   \n",
      "2  Won 5 Oscars, 38 wins & 15 nominations total   \n",
      "3   Won 1 Oscar, 14 wins & 20 nominations total   \n",
      "4    Won 1 Oscar, 3 wins & 11 nominations total   \n",
      "\n",
      "                                              genres    languages  \n",
      "0  ['Monster Horror', 'Sea Adventure', 'Survival'...  ['English']  \n",
      "1  ['B-Horror', 'Dark Comedy', 'Parody', 'Raunchy...  ['English']  \n",
      "2  ['Medical Drama', 'Psychological Drama', 'Drama']  ['English']  \n",
      "3  ['Dark Comedy', 'Heist', 'True Crime', 'Biogra...  ['English']  \n",
      "4                      ['Satire', 'Comedy', 'Drama']  ['English']  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = kagglehub.dataset_download(\"raedaddala/top-500-600-movies-of-each-year-from-1960-to-2024\")\n",
    "\n",
    "csv_filename = os.path.join(dataset_path, \"final_dataset.csv\")\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "print(\"Loaded dataset from:\", csv_filename)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_duration_to_minutes(duration):\n",
    "    if pd.isna(duration):\n",
    "        return 0  \n",
    "    if isinstance(duration, int):\n",
    "        return duration  \n",
    "\n",
    "    total_minutes = 0\n",
    "    parts = duration.split()\n",
    "    for part in parts:\n",
    "        if 'h' in part:\n",
    "            total_minutes += int(part[:-1]) * 60 \n",
    "        elif 'm' in part:\n",
    "            total_minutes += int(part[:-1])  \n",
    "    return total_minutes\n",
    "\n",
    "df['duration'] = df['duration'].apply(convert_duration_to_minutes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mansi\\AppData\\Local\\Temp\\ipykernel_2468\\1653071518.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\Mansi\\AppData\\Local\\Temp\\ipykernel_2468\\1653071518.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    if df[col].dtype in ['int', 'int64', 'float64']: \n",
    "        df[col].fillna(0, inplace=True)\n",
    "    else:\n",
    "        df[col].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_list(cell):\n",
    "    if pd.isnull(cell) or cell == \"\":\n",
    "        return []\n",
    "    try:\n",
    "        return ast.literal_eval(cell)  \n",
    "    except (ValueError, SyntaxError):\n",
    "        return cell.split(\", \")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def get_cleaned_name_string(name_list):\n",
    "    cleaned_names = []\n",
    "    if isinstance(name_list, str):\n",
    "        if name_list != \"\":\n",
    "            name_list = ast.literal_eval(name_list)\n",
    "    if not isinstance(name_list, list):\n",
    "        name_list = []\n",
    "    for name in name_list:\n",
    "        cleaned_name = re.sub(r'[^a-zA-Z]', '', name).lower()\n",
    "        cleaned_names.append(cleaned_name)\n",
    "    return ' '.join(cleaned_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_locations(locations):\n",
    "    cleaned_locations = []\n",
    "    if isinstance(locations, str):\n",
    "        if locations != \"\":\n",
    "            locations = ast.literal_eval(locations)\n",
    "    if not isinstance(locations, list):\n",
    "        locations = []\n",
    "    for location in locations:\n",
    "        cleaned_location = desc_cleaning(location)\n",
    "        cleaned_locations.append(cleaned_location)\n",
    "    return ' '.join(cleaned_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mansi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mansi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_cleaning(desc):\n",
    "    desc = desc.lower()\n",
    "    desc = re.sub(r'[^a-z\\s]', '', desc)\n",
    "    words = word_tokenize(desc)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_cleaning(name):\n",
    "    name = name.lower()\n",
    "    words = str.split(name)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_doc(row):\n",
    "    new_name = name_cleaning(row['title'])\n",
    "    new_desc = desc_cleaning(row['description'])\n",
    "    new_stars = get_cleaned_name_string(row['stars'])\n",
    "    new_directors = get_cleaned_name_string(row['directors'])\n",
    "    new_genres = get_cleaned_locations(row['genres'])\n",
    "    new_production = get_cleaned_locations(row['production_companies'])\n",
    "    new_filming_locations = get_cleaned_locations(row['filming_locations'])\n",
    "    new_language = get_cleaned_name_string(row['languages'])\n",
    "    new_countries = get_cleaned_name_string(row['countries_origin'])\n",
    "    doc_list = [new_name, new_desc, new_stars, new_directors, new_genres, new_production, new_filming_locations, new_language, new_countries]\n",
    "    return str.join(\" \", doc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m     movie_data_dict[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocs\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_cleaned_doc(row)\n\u001b[0;32m      8\u001b[0m     }\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Second iteration to add other movie data\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m, in \u001b[0;36mget_cleaned_doc\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      5\u001b[0m new_directors \u001b[38;5;241m=\u001b[39m get_cleaned_name_string(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirectors\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      6\u001b[0m new_genres \u001b[38;5;241m=\u001b[39m get_cleaned_locations(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m new_production \u001b[38;5;241m=\u001b[39m get_cleaned_locations(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduction_companies\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m new_filming_locations \u001b[38;5;241m=\u001b[39m get_cleaned_locations(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilming_locations\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m new_language \u001b[38;5;241m=\u001b[39m get_cleaned_name_string(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguages\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mget_cleaned_locations\u001b[1;34m(locations)\u001b[0m\n\u001b[0;32m      7\u001b[0m     locations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m location \u001b[38;5;129;01min\u001b[39;00m locations:\n\u001b[1;32m----> 9\u001b[0m     cleaned_location \u001b[38;5;241m=\u001b[39m desc_cleaning(location)\n\u001b[0;32m     10\u001b[0m     cleaned_locations\u001b[38;5;241m.\u001b[39mappend(cleaned_location)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_locations)\n",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m, in \u001b[0;36mdesc_cleaning\u001b[1;34m(desc)\u001b[0m\n\u001b[0;32m      3\u001b[0m desc \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-z\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, desc)\n\u001b[0;32m      4\u001b[0m words \u001b[38;5;241m=\u001b[39m word_tokenize(desc)\n\u001b[1;32m----> 5\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m filtered_words \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered_words)\n",
      "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw(fileids))\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(f) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_root\u001b[38;5;241m.\u001b[39mjoin(file)\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\Lib\\site-packages\\nltk\\data.py:333\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    332\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FileSystemPathPointer(_path)\n",
      "File \u001b[1;32mc:\\Users\\Mansi\\anaconda3\\Lib\\site-packages\\nltk\\data.py:310\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_path):\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "movie_data_dict = {}\n",
    "\n",
    "# First iteration to store cleaned document data\n",
    "for _, row in df.iterrows():\n",
    "    id = row[\"id\"]\n",
    "    movie_data_dict[id] = {\n",
    "        \"docs\": get_cleaned_doc(row)\n",
    "    }\n",
    "\n",
    "# Second iteration to add other movie data\n",
    "for _, row in df.iterrows():\n",
    "    id = row[\"id\"]\n",
    "    if id in movie_data_dict:\n",
    "        movie_data_dict[id].update({\n",
    "            \"title\": row[\"title\"],\n",
    "            \"year\": row[\"year\"],\n",
    "            \"duration\": row[\"duration\"],\n",
    "            \"MPA\": row[\"MPA\"],\n",
    "            \"rating\": row[\"rating\"],\n",
    "            \"votes\": row[\"votes\"],\n",
    "            \"meta_score\": row[\"meta_score\"],\n",
    "            \"description\": row[\"description\"],\n",
    "            \"Movie_Link\": row[\"Movie_Link\"],\n",
    "            \"writers\": parse_list(row[\"writers\"]),\n",
    "            \"directors\": parse_list(row[\"directors\"]),\n",
    "            \"stars\": parse_list(row[\"stars\"]),\n",
    "            \"budget\": row[\"budget\"],\n",
    "            \"opening_weekend_gross\": row[\"opening_weekend_gross\"],\n",
    "            \"gross_worldwide\": row[\"gross_worldwide\"],\n",
    "            \"gross_us_canada\": row[\"gross_us_canada\"],\n",
    "            \"release_date\": row[\"release_date\"],\n",
    "            \"countries_origin\": parse_list(row[\"countries_origin\"]),\n",
    "            \"filming_locations\": parse_list(row[\"filming_locations\"]),\n",
    "            \"production_companies\": parse_list(row[\"production_companies\"]),\n",
    "            \"awards_content\": parse_list(row[\"awards_content\"]),\n",
    "            \"genres\": parse_list(row[\"genres\"]),\n",
    "            \"languages\": parse_list(row[\"languages\"])\n",
    "        })\n",
    "\n",
    "# Print first 5 movie IDs and their data\n",
    "first_5_keys = list(movie_data_dict.keys())[:5]\n",
    "for key in first_5_keys:\n",
    "    print(f\"Movie ID: {key}, Data: {movie_data_dict[key]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to ./../cleaned_database/cleaned_final_dataset3.csv\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = pd.DataFrame.from_dict(movie_data_dict, orient=\"index\")\n",
    "df_cleaned.to_csv(\"./../cleaned_database/cleaned_final_dataset3.csv\", index_label=\"id\")\n",
    "print(\"Dictionary saved to ./../cleaned_database/cleaned_final_dataset3.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
